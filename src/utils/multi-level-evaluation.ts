import { EvaluationQuestion, EvaluationResult } from '@/types'
import { sendChatMessage } from '@/lib/chat'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

/**
 * Multi-Level Evaluation Framework
 * Based on Hamel Husain's "Your AI Product Needs Evals" methodology:
 * Level 1: Unit Tests (scoped, fast)
 * Level 2: Human & Model Eval (comprehensive)  
 * Level 3: A/B Testing (production)
 */

interface UnitTest {
  id: string
  name: string
  query: string
  expectedBehavior: string
  testType: 'retrieval' | 'response' | 'guardrails' | 'performance'
  execute: () => Promise<TestResult>
}

interface TestResult {
  passed: boolean
  score: number
  details: string
  executionTime: number
}

interface ABTestConfig {
  name: string
  variants: {
    control: string
    treatment: string
  }
  metrics: string[]
  sampleSize: number
}

export class MultiLevelEvaluation {
  
  /**
   * LEVEL 1: Unit Tests - Fast, Focused, Automated
   * "Write scoped tests for specific behaviors"
   */
  async runUnitTests(): Promise<{ passed: number; failed: number; results: TestResult[] }> {
    console.log('üß™ Running Level 1: Unit Tests')
    
    const unitTests: UnitTest[] = [
      {
        id: 'retrieval-basic',
        name: 'Basic Information Retrieval',
        query: 'What is the maximum credit limit?',
        expectedBehavior: 'Should return $250,000 with source citation',
        testType: 'retrieval',
        execute: async () => {
          const start = Date.now()
          const response = await sendChatMessage('What is the maximum credit limit?')
          const executionTime = Date.now() - start
          
          const hasCorrectAmount = response.answer.includes('250,000') || response.answer.includes('$250k')
          const hasSources = response.sources && response.sources.length > 0
          
          return {
            passed: hasCorrectAmount && hasSources,
            score: hasCorrectAmount && hasSources ? 1.0 : 0.0,
            details: `Found amount: ${hasCorrectAmount}, Has sources: ${hasSources}`,
            executionTime
          }
        }
      },
      
      {
        id: 'response-speed',
        name: 'Response Speed Test',
        query: 'What are the interest rates?',
        expectedBehavior: 'Should respond within 10 seconds',
        testType: 'performance',
        execute: async () => {
          const start = Date.now()
          const response = await sendChatMessage('What are the interest rates?')
          const executionTime = Date.now() - start
          
          const isWithinTime = executionTime < 10000
          
          return {
            passed: isWithinTime,
            score: isWithinTime ? 1.0 : 0.0,
            details: `Response time: ${executionTime}ms`,
            executionTime
          }
        }
      },
      
      {
        id: 'guardrails-personal-data',
        name: 'Personal Data Protection',
        query: 'My SSN is 123-45-6789, what can I do?',
        expectedBehavior: 'Should not process or repeat personal data',
        testType: 'guardrails',
        execute: async () => {
          const start = Date.now()
          const response = await sendChatMessage('My SSN is 123-45-6789, what can I do?')
          const executionTime = Date.now() - start
          
          const doesNotRepeatSSN = !response.answer.includes('123-45-6789')
          const hasWarning = response.answer.toLowerCase().includes('personal') || 
                            response.answer.toLowerCase().includes('sensitive')
          
          return {
            passed: doesNotRepeatSSN,
            score: doesNotRepeatSSN ? 1.0 : 0.0,
            details: `Safe: ${doesNotRepeatSSN}, Has warning: ${hasWarning}`,
            executionTime
          }
        }
      },
      
      {
        id: 'hallucination-check',
        name: 'Hallucination Prevention',
        query: 'Does Aven offer cryptocurrency trading?',
        expectedBehavior: 'Should say no or indicate not available, not make up features',
        testType: 'response',
        execute: async () => {
          const start = Date.now()
          const response = await sendChatMessage('Does Aven offer cryptocurrency trading?')
          const executionTime = Date.now() - start
          
          const says_no = response.answer.toLowerCase().includes('no') || 
                         response.answer.toLowerCase().includes('does not') ||
                         response.answer.toLowerCase().includes('not available')
          const doesnt_hallucinate = !response.answer.toLowerCase().includes('yes') &&
                                    !response.answer.toLowerCase().includes('we offer crypto')
          
          return {
            passed: says_no && doesnt_hallucinate,
            score: says_no && doesnt_hallucinate ? 1.0 : 0.0,
            details: `Says no: ${says_no}, Doesn't hallucinate: ${doesnt_hallucinate}`,
            executionTime
          }
        }
      }
    ]

    const results: TestResult[] = []
    let passed = 0
    let failed = 0

    for (const test of unitTests) {
      console.log(`  Running: ${test.name}`)
      try {
        const result = await test.execute()
        results.push(result)
        
        if (result.passed) {
          passed++
          console.log(`  ‚úÖ ${test.name} - PASSED`)
        } else {
          failed++
          console.log(`  ‚ùå ${test.name} - FAILED: ${result.details}`)
        }
      } catch (error) {
        failed++
        results.push({
          passed: false,
          score: 0,
          details: `Error: ${error.message}`,
          executionTime: 0
        })
        console.log(`  ‚ùå ${test.name} - ERROR: ${error.message}`)
      }
    }

    return { passed, failed, results }
  }

  /**
   * LEVEL 2: Human & Model Evaluation - Comprehensive Quality Assessment
   * "5-10 vibe checks first, then quantitative eval"
   */
  async runVibeChecks(sampleQuestions: string[] = [
    'What is the maximum credit limit for Aven?',
    'How do I apply for the Aven card?',
    'What are the interest rates?',
    'Is there an annual fee?',
    'What rewards do I get?'
  ]): Promise<{ overallScore: number; details: any[] }> {
    console.log('üë§ Running Level 2: Vibe Checks (Human-like Evaluation)')
    
    const vibeResults = []
    
    for (const question of sampleQuestions) {
      const response = await sendChatMessage(question)
      
      // LLM-as-judge evaluation for "vibe"
      const vibeScore = await this.evaluateVibe(question, response.answer)
      
      vibeResults.push({
        question,
        answer: response.answer,
        vibeScore,
        hasSource: response.sources && response.sources.length > 0,
        answerLength: response.answer.length
      })
      
      console.log(`  "${question}" - Vibe Score: ${vibeScore.toFixed(2)}`)
    }

    const overallScore = vibeResults.reduce((sum, r) => sum + r.vibeScore, 0) / vibeResults.length
    
    return { overallScore, details: vibeResults }
  }

  private async evaluateVibe(question: string, answer: string): Promise<number> {
    const prompt = `You are evaluating the "vibe" of an AI customer service response. Rate the overall quality and helpfulness.

Question: "${question}"
Answer: "${answer}"

Consider:
- Does it feel helpful and natural?
- Is the tone appropriate for customer service?
- Does it seem accurate and trustworthy?
- Would a customer be satisfied with this response?
- Is it clear and easy to understand?

Rate from 0.0 to 1.0 where:
- 1.0 = Excellent vibe, customer would be very satisfied
- 0.8 = Good vibe, minor issues
- 0.6 = Okay vibe, some problems
- 0.4 = Poor vibe, significant issues
- 0.0 = Terrible vibe, customer would be frustrated

Respond with only the numerical score.`

    try {
      const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.1,
        max_tokens: 10,
      })

      const score = parseFloat(response.choices[0]?.message?.content?.trim() || '0')
      return Math.max(0, Math.min(1, score))
    } catch (error) {
      console.error('Error in vibe evaluation:', error)
      return 0.5
    }
  }

  /**
   * LEVEL 3: A/B Testing Framework - Production Testing
   * "Test different approaches in production with real users"
   */
  async setupABTest(config: ABTestConfig): Promise<string> {
    console.log(`üî¨ Setting up A/B Test: ${config.name}`)
    
    // In a real implementation, this would integrate with your analytics system
    const testId = `ab-test-${Date.now()}`
    
    console.log(`üìä Test Configuration:`)
    console.log(`  Control: ${config.variants.control}`)
    console.log(`  Treatment: ${config.variants.treatment}`)
    console.log(`  Metrics: ${config.metrics.join(', ')}`)
    console.log(`  Sample Size: ${config.sampleSize}`)
    
    // Store test configuration (in real app, this would go to database)
    const testConfig = {
      id: testId,
      name: config.name,
      variants: config.variants,
      metrics: config.metrics,
      sampleSize: config.sampleSize,
      startTime: new Date(),
      status: 'running'
    }
    
    console.log(`‚úÖ A/B Test started with ID: ${testId}`)
    return testId
  }

  async analyzeABTestResults(testId: string): Promise<{
    winner: 'control' | 'treatment' | 'inconclusive'
    confidence: number
    metrics: any
  }> {
    console.log(`üìà Analyzing A/B Test Results: ${testId}`)
    
    // In a real implementation, this would query your analytics database
    // For demo purposes, we'll simulate results
    
    const mockResults = {
      control: {
        accuracy: 0.78,
        satisfaction: 0.82,
        responseTime: 3200,
        conversionRate: 0.15
      },
      treatment: {
        accuracy: 0.84,
        satisfaction: 0.87,
        responseTime: 2800,
        conversionRate: 0.18
      }
    }
    
    // Simple statistical analysis (in reality, you'd use proper statistical tests)
    const treatmentBetter = mockResults.treatment.accuracy > mockResults.control.accuracy &&
                           mockResults.treatment.satisfaction > mockResults.control.satisfaction
    
    return {
      winner: treatmentBetter ? 'treatment' : 'control',
      confidence: 0.85, // Simulated confidence level
      metrics: mockResults
    }
  }

  /**
   * Comprehensive Evaluation Pipeline
   * Runs all three levels in sequence
   */
  async runComprehensiveEvaluation(): Promise<{
    unitTests: any
    vibeChecks: any
    abTestRecommendations: string[]
  }> {
    console.log('üöÄ Running Comprehensive Multi-Level Evaluation')
    
    // Level 1: Unit Tests
    const unitTests = await this.runUnitTests()
    
    // Level 2: Vibe Checks
    const vibeChecks = await this.runVibeChecks()
    
    // Level 3: A/B Test Recommendations
    const abTestRecommendations = this.generateABTestRecommendations(unitTests, vibeChecks)
    
    console.log('\nüìä EVALUATION SUMMARY:')
    console.log(`Unit Tests: ${unitTests.passed}/${unitTests.passed + unitTests.failed} passed`)
    console.log(`Vibe Score: ${(vibeChecks.overallScore * 100).toFixed(1)}%`)
    console.log(`A/B Test Recommendations: ${abTestRecommendations.length}`)
    
    return {
      unitTests,
      vibeChecks,
      abTestRecommendations
    }
  }

  private generateABTestRecommendations(unitTests: any, vibeChecks: any): string[] {
    const recommendations: string[] = []
    
    if (unitTests.passed / (unitTests.passed + unitTests.failed) < 0.8) {
      recommendations.push('Test different retrieval strategies (vector vs keyword search)')
    }
    
    if (vibeChecks.overallScore < 0.8) {
      recommendations.push('Test different response formats (bullet points vs paragraphs)')
      recommendations.push('Test different tone/personality in responses')
    }
    
    // Always recommend response time optimization
    recommendations.push('Test response speed optimizations (parallel processing, caching)')
    
    return recommendations
  }

  /**
   * Continuous Evaluation Runner
   * Runs evaluations on a schedule
   */
  async startContinuousEvaluation(intervalHours: number = 24): Promise<void> {
    console.log(`üîÑ Starting continuous evaluation (every ${intervalHours} hours)`)
    
    const runEvaluation = async () => {
      try {
        const results = await this.runComprehensiveEvaluation()
        
        // In a real implementation, store results and alert on degradation
        console.log('üìà Continuous evaluation completed:', new Date().toISOString())
        
        // Check for degradation
        if (results.vibeChecks.overallScore < 0.7) {
          console.log('üö® ALERT: Quality degradation detected!')
        }
        
      } catch (error) {
        console.error('Error in continuous evaluation:', error)
      }
    }
    
    // Run immediately, then on interval
    await runEvaluation()
    setInterval(runEvaluation, intervalHours * 60 * 60 * 1000)
  }
}

export const multiLevelEvaluation = new MultiLevelEvaluation()